{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azou/.pyenv/versions/3.11.7/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/azou/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import rasterio  # For geospatial data\n",
    "from datetime import datetime\n",
    "import faiss\n",
    "\n",
    "# Define directories\n",
    "image_dir = \"data/geospatial\"\n",
    "csv_dir = \"data/csv\"\n",
    "text_dir = \"data/text\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'driver': 'GTiff', 'dtype': 'int16', 'nodata': None, 'width': 4096, 'height': 4096, 'count': 12, 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'transform': Affine(0.0025, 0.0, 40.965,\n",
      "       0.0, -0.0025, 11.99)}\n",
      "Data shape: (4096, 4096)\n",
      "CRS: EPSG:4326\n",
      "Metadata: {'driver': 'GTiff', 'dtype': 'int16', 'nodata': None, 'width': 4096, 'height': 5475, 'count': 12, 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'transform': Affine(0.0025, 0.0, 40.965,\n",
      "       0.0, -0.0025, 1.75)}\n",
      "Data shape: (5475, 4096)\n",
      "CRS: EPSG:4326\n",
      "Metadata: {'driver': 'GTiff', 'dtype': 'int16', 'nodata': None, 'width': 4181, 'height': 5475, 'count': 12, 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'transform': Affine(0.0025, 0.0, 51.205000000000005,\n",
      "       0.0, -0.0025, 1.75)}\n",
      "Data shape: (5475, 4181)\n",
      "CRS: EPSG:4326\n",
      "Metadata: {'driver': 'GTiff', 'dtype': 'int16', 'nodata': None, 'width': 4181, 'height': 4096, 'count': 12, 'crs': CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'transform': Affine(0.0025, 0.0, 51.205000000000005,\n",
      "       0.0, -0.0025, 11.99)}\n",
      "Data shape: (4096, 4181)\n",
      "CRS: EPSG:4326\n",
      "Successfully processed tif data of length:  4\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "image_data = {}  # Dictionary to store images by location and date\n",
    "for filename in os.listdir(image_dir):\n",
    "    with rasterio.open(os.path.join(image_dir, filename)) as src:\n",
    "        raster_data = src.read(1)  # Read the first band\n",
    "        metadata = src.meta\n",
    "\n",
    "        print(\"Metadata:\", metadata)\n",
    "        print(\"Data shape:\", raster_data.shape)\n",
    "\n",
    "        # Example: Show coordinate reference system\n",
    "        crs = src.crs\n",
    "        print(\"CRS:\", crs)\n",
    "\n",
    "        name = filename.split(\".\")[0]\n",
    "        image_data[(crs, name)] = raster_data\n",
    "\n",
    "print(\"Successfully processed tif data of length: \", len(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azou/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/azou/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added embeddings for 4 data\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained ResNet model\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model = torch.nn.Sequential(\n",
    "    *(list(resnet_model.children())[:-1])\n",
    ")  # Remove final classification layer\n",
    "resnet_model = resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "\n",
    "# Define preprocessing\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Image embedding function\n",
    "def get_image_embedding(image_array):\n",
    "    # Ensure raster data is in a suitable format for preprocessing\n",
    "    if image_array.dtype != np.uint8:\n",
    "        image_array = (\n",
    "            255 * (image_array - image_array.min()) / (image_array.ptp())\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "    # Stack the image to 3 channels to match ResNet's input\n",
    "    image_array = np.stack([image_array] * 3, axis=-1)\n",
    "    processed_image = (\n",
    "        preprocess(image_array).unsqueeze(0).to(device)\n",
    "    )  # Move to GPU if available\n",
    "\n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet_model(processed_image).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# Create embeddings for each image and store in a dictionary\n",
    "image_embeddings = {}\n",
    "for (crs, name), raster_data in image_data.items():\n",
    "    embedding = get_image_embedding(raster_data)\n",
    "    image_embeddings[(crs, name)] = embedding\n",
    "\n",
    "print(f\"Successfully added embeddings for {len(image_embeddings)} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images indexed: 4\n",
      "Top 5 similar images: [((CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'drought-vulnerability-summary_0'), 0.0), ((CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'drought-vulnerability-summary_2'), 157.74901), ((CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'drought-vulnerability-summary_3'), 158.48993), ((CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'drought-vulnerability-summary_1'), 158.805), ((CRS.from_wkt('GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'), 'drought-vulnerability-summary_2'), 3.4028235e+38)]\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to a numpy array for FAISS and keep a list of metadata\n",
    "embedding_matrix = np.array(list(image_embeddings.values())).astype(\"float32\")\n",
    "image_keys = list(image_embeddings.keys())  # Store the (crs, name) keys for reference\n",
    "\n",
    "# Initialize FAISS index for L2 distance (Euclidean distance)\n",
    "dimension = embedding_matrix.shape[1]  # Number of features in each embedding\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_matrix)  # Add all embeddings to the index\n",
    "\n",
    "# Check that the index has been populated\n",
    "print(\"Number of images indexed:\", index.ntotal)\n",
    "\n",
    "# Semantic search function to retrieve top-k similar images\n",
    "def semantic_search(query_image_array, k=5):\n",
    "    # Generate embedding for the query image\n",
    "    query_embedding = get_image_embedding(query_image_array).astype(\"float32\")\n",
    "\n",
    "    # Perform FAISS search\n",
    "    D, I = index.search(np.array([query_embedding]), k)  # D = distances, I = indices\n",
    "\n",
    "    # Retrieve top-k metadata based on indices\n",
    "    top_k_files = [image_keys[idx] for idx in I[0]]\n",
    "    top_k_distances = D[0]\n",
    "\n",
    "    return list(zip(top_k_files, top_k_distances))  # Return metadata with distances\n",
    "\n",
    "\n",
    "# Example search for image similarity (using an existing image as query)\n",
    "query_image_key = list(image_data.keys())[0]  # Taking the first image as an example\n",
    "query_image = image_data[query_image_key]  # Get the raster data\n",
    "\n",
    "results = semantic_search(query_image, k=5)\n",
    "print(\"Top 5 similar images:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Endpoint for semantic image search (using an image file as input)\n",
    "@app.post(\"/search_image/\")\n",
    "async def search_image(k: int = 5):\n",
    "    try:\n",
    "        # Process image (this part would require proper decoding if input is raw bytes)\n",
    "        results = semantic_search(query_image, k=5)\n",
    "        return {\"top_matches\": results}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Run with `uvicorn <filename>:app --reload`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
